{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74d1e3b8-39e4-46da-9373-e6eb2e7a557c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Project 2: Incremental ETL with CDC & SCD in Databricks  \n",
    "\n",
    "In **Project 1**, we implemented a baseline medallion architecture (Bronze → Silver → Gold) on static CSV files of e-commerce data, focusing on cleaning and structuring. Those original CSVs remain available in Project 1 for reference.  \n",
    "\n",
    "This follow-up project extends that work into a more realistic **incremental pipeline**, introducing advanced data engineering concepts:  \n",
    "- **Incremental loads (daily feeds)** instead of static files.  \n",
    "- **CDC (Change Data Capture)** for customers → keep only the latest state.  \n",
    "- **SCD Type-2 (Slowly Changing Dimension)** for products → preserve history with valid date ranges.  \n",
    "- **Fact tables with point-in-time joins** to attach the correct product version to each order.  \n",
    "- **Incremental aggregates (Gold layer)** for KPIs such as revenue, order counts, and average order value.  \n",
    "\n",
    "The pipeline is built using the **Databricks Lakehouse (PySpark + Delta Lake)** and simulates daily e-commerce data feeds.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab094d7b-6a6b-4b82-83fa-f321af3c1c09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 1 – Environment Setup\n",
    "We start by selecting our working **catalog** (`workspace`) and **schema** (`default`) so all new tables are created in a consistent namespace.  \n",
    "We also create a **`gold` schema** to hold final reporting tables.  \n",
    "\n",
    "Finally, we configure Spark’s **time parser policy** to `LEGACY`. This makes date parsing more forgiving across mixed formats (e.g., `yyyy-MM-dd`, `MM/dd/yyyy`), which is important when handling messy raw data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "29b3e989-35c0-4a3b-a693-0b50e014edef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 1: Setup\n",
    "spark.sql(\"USE CATALOG workspace\")\n",
    "spark.sql(\"USE SCHEMA default\")\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS gold\")\n",
    "\n",
    "# Safer date parsing across mixed formats\n",
    "spark.conf.set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b991986-069d-4acf-92c8-cc00f09a829f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 2 – Simulate Day 1 Incoming Data  \n",
    "\n",
    "Since the free Databricks edition does not allow automatic ingestion, we **simulate daily feeds** by manually creating small DataFrames for:  \n",
    "- Customers  \n",
    "- Products  \n",
    "- Orders  \n",
    "- Order Items  \n",
    "\n",
    "This represents the **Day 1 incremental load**.  \n",
    "\n",
    "Each DataFrame is written into a **Bronze staging table** (Delta format, `*_inc` suffix). These tables act as the raw incoming data for that day, ready to be merged into the Silver layer.  \n",
    "\n",
    "\uD83D\uDC49 Expected outcome: 4 new Bronze tables are created (`bronze_customers_inc`, `bronze_products_inc`, `bronze_orders_inc`, `bronze_order_items_inc`).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c0f6494-edd7-4409-99c0-d85866377d09",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 2: Simulate Day 1 incoming data\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "cust_day1 = spark.createDataFrame([\n",
    "    (1, \"Lucas Garcia\",  \"WA\",  \"2022-05-23\"),\n",
    "    (2, \"Oliver Martin\", \"SA\",  \"2022-06-13\"),\n",
    "    (3, \"Leo Brown\",     \"ACT\", \"2023-07-24\"),\n",
    "], [\"customer_id\",\"customer_name\",\"region\",\"signup_date\"])\n",
    "\n",
    "prod_day1 = spark.createDataFrame([\n",
    "    (1, \"Sports Item 1\", \"SPORTS\",          60.11),\n",
    "    (2, \"Sports Item 2\", \"SPORTS\",         141.14),\n",
    "    (3, \"Home Item 3\",   \"HOME & KITCHEN\",  74.07),\n",
    "], [\"product_id\",\"product_name\",\"category\",\"list_price\"])\n",
    "\n",
    "orders_day1 = spark.createDataFrame([\n",
    "    (1001, 1, \"2025-03-29\", \"SHIPPED\"),\n",
    "    (1002, 2, \"2025-03-29\", \"SHIPPED\"),\n",
    "], [\"order_id\",\"customer_id\",\"order_date\",\"status\"])\n",
    "\n",
    "items_day1 = spark.createDataFrame([\n",
    "    (1,1001,1,2, 60.11),\n",
    "    (2,1002,2,1,141.14),\n",
    "], [\"order_item_id\",\"order_id\",\"product_id\",\"quantity\",\"unit_price\"])\n",
    "\n",
    "# Write to Bronze incremental staging\n",
    "cust_day1.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze_customers_inc\")\n",
    "prod_day1.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze_products_inc\")\n",
    "orders_day1.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze_orders_inc\")\n",
    "items_day1.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze_order_items_inc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f702ab22-a23d-4da6-b6c3-af0379cb61a1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 3 – Silver Customers (CDC Type-1 Upsert)\n",
    "\n",
    "**What this does:**  \n",
    "We keep a **latest snapshot** of each customer. If a customer already exists, we **update** their details; if not, we **insert** them. This is classic **CDC Type-1** (overwrite, no history).\n",
    "\n",
    "**How it works:**  \n",
    "- Create the target Delta table `silver_customers_cdc` (once).  \n",
    "- Read today’s increment (`bronze_customers_inc`) and do light cleaning:\n",
    "  - cast `customer_id` to INT  \n",
    "  - `TRIM` names, `UPPER(TRIM(region))`  \n",
    "- Use `MERGE INTO` on `customer_id`:\n",
    "  - **MATCHED** → `UPDATE` (refresh fields)\n",
    "  - **NOT MATCHED** → `INSERT` (new customer)\n",
    "\n",
    "**Why Type-1 here:**  \n",
    "We only need the **current** customer attributes (no history). It keeps the table simple for joining in Gold.\n",
    "\n",
    "**Expected outcome:**  \n",
    "`silver_customers_cdc` contains **one row per `customer_id`**, reflecting the latest name/region/signup_date. The `display(...)` shows the cleaned, up-to-date snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88d56b69-0069-429b-89d1-77d736c33f38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>customer_id</th><th>customer_name</th><th>region</th><th>signup_date</th></tr></thead><tbody><tr><td>1</td><td>Lucas Garcia</td><td>WA</td><td>2022-05-23</td></tr><tr><td>2</td><td>Oliver Martin</td><td>SA</td><td>2022-06-13</td></tr><tr><td>3</td><td>Leo Brown</td><td>ACT</td><td>2023-07-24</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Lucas Garcia",
         "WA",
         "2022-05-23"
        ],
        [
         2,
         "Oliver Martin",
         "SA",
         "2022-06-13"
        ],
        [
         3,
         "Leo Brown",
         "ACT",
         "2023-07-24"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "region",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "signup_date",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 3: Silver Customers CDC (Type-1)\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS silver_customers_cdc (\n",
    "  customer_id INT,\n",
    "  customer_name STRING,\n",
    "  region STRING,\n",
    "  signup_date STRING\n",
    ") USING delta\n",
    "\"\"\")\n",
    "\n",
    "# Upsert (insert new, update existing)\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO silver_customers_cdc AS tgt\n",
    "USING (\n",
    "  SELECT\n",
    "    CAST(customer_id AS INT) AS customer_id,\n",
    "    TRIM(customer_name) AS customer_name,\n",
    "    UPPER(TRIM(region)) AS region,\n",
    "    signup_date\n",
    "  FROM bronze_customers_inc\n",
    ") AS src\n",
    "ON tgt.customer_id = src.customer_id\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  tgt.customer_name = src.customer_name,\n",
    "  tgt.region        = src.region,\n",
    "  tgt.signup_date   = src.signup_date\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "\n",
    "#check customer CDC\n",
    "display(\n",
    "  spark.table(\"silver_customers_cdc\")\n",
    "       .orderBy(\"customer_id\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17724bc9-214b-4f70-ab99-f395a90f72ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 4 – Silver Products (SCD Type-2 with History)\n",
    "\n",
    "**What this does:**  \n",
    "We track **product changes over time** (name/category/price). Instead of overwriting, we **keep history** by closing the old row and inserting a new “current” row. This is **SCD Type-2**.\n",
    "\n",
    "**How it works:**  \n",
    "1) Create `silver_products_scd` with versioning columns:  \n",
    "   - `valid_from`, `valid_to`, `is_current` (true = active version).  \n",
    "2) Build two helper views:  \n",
    "   - `current_products` → only active versions (`is_current = true`).  \n",
    "   - `incoming_products` → today’s cleaned feed from Bronze.  \n",
    "3) **Close changed rows (4a):**  \n",
    "   - If an incoming product differs from the current one, update the current row: set `valid_to = current_date()` and `is_current = false`.  \n",
    "4) **Insert new versions (4b):**  \n",
    "   - Insert a row for any **new product** or **changed product** with `valid_from = current_date()`, `valid_to = 9999-12-31`, `is_current = true`.\n",
    "\n",
    "**Why SCD2 here:**  \n",
    "Product attributes (especially **price**) change, and we need correct values **as of the order date** later in Gold (point-in-time joins).\n",
    "\n",
    "**Expected outcome:**  \n",
    "`silver_products_scd` shows one **current** row per product plus older **historical** rows. Sorting by `product_id, valid_from` reveals the version timeline.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "617b977d-c8a2-41ec-b797-8544af4c6d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>product_name</th><th>category</th><th>list_price</th><th>valid_from</th><th>valid_to</th><th>is_current</th></tr></thead><tbody><tr><td>1</td><td>Sports Item 1</td><td>SPORTS</td><td>60.11</td><td>2025-09-29</td><td>9999-12-31</td><td>true</td></tr><tr><td>2</td><td>Sports Item 2</td><td>SPORTS</td><td>141.14</td><td>2025-09-29</td><td>9999-12-31</td><td>true</td></tr><tr><td>3</td><td>Home Item 3</td><td>HOME & KITCHEN</td><td>74.07</td><td>2025-09-29</td><td>9999-12-31</td><td>true</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1,
         "Sports Item 1",
         "SPORTS",
         60.11,
         "2025-09-29",
         "9999-12-31",
         true
        ],
        [
         2,
         "Sports Item 2",
         "SPORTS",
         141.14,
         "2025-09-29",
         "9999-12-31",
         true
        ],
        [
         3,
         "Home Item 3",
         "HOME & KITCHEN",
         74.07,
         "2025-09-29",
         "9999-12-31",
         true
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "list_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "valid_from",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "valid_to",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "is_current",
         "type": "\"boolean\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 4: Silver Products SCD2\n",
    "\n",
    "# Create SCD2 table if not exists\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS silver_products_scd (\n",
    "  product_id INT,\n",
    "  product_name STRING,\n",
    "  category STRING,\n",
    "  list_price DOUBLE,\n",
    "  valid_from DATE,\n",
    "  valid_to   DATE,\n",
    "  is_current BOOLEAN\n",
    ") USING delta\n",
    "\"\"\")\n",
    "\n",
    "# Helper: current rows\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW current_products AS\n",
    "SELECT * FROM silver_products_scd WHERE is_current = true\n",
    "\"\"\")\n",
    "\n",
    "# Incoming cleaned\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW incoming_products AS\n",
    "SELECT\n",
    "  CAST(product_id AS INT) AS product_id,\n",
    "  TRIM(product_name) AS product_name,\n",
    "  UPPER(TRIM(category)) AS category,\n",
    "  CAST(list_price AS DOUBLE) AS list_price\n",
    "FROM bronze_products_inc\n",
    "\"\"\")\n",
    "\n",
    "# 4a) Close changed rows\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO silver_products_scd AS tgt\n",
    "USING (\n",
    "  SELECT c.product_id\n",
    "  FROM current_products c\n",
    "  JOIN incoming_products i ON c.product_id = i.product_id\n",
    "  WHERE c.is_current = true AND (\n",
    "    c.product_name <> i.product_name OR\n",
    "    c.category     <> i.category     OR\n",
    "    c.list_price   <> i.list_price\n",
    "  )\n",
    ") AS chg\n",
    "ON tgt.product_id = chg.product_id AND tgt.is_current = true\n",
    "WHEN MATCHED THEN UPDATE SET\n",
    "  tgt.valid_to   = current_date(),\n",
    "  tgt.is_current = false\n",
    "\"\"\")\n",
    "\n",
    "# 4b) Insert new versions (new or changed)\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO silver_products_scd\n",
    "SELECT\n",
    "  i.product_id, i.product_name, i.category, i.list_price,\n",
    "  current_date() AS valid_from,\n",
    "  DATE '9999-12-31' AS valid_to,\n",
    "  true AS is_current\n",
    "FROM incoming_products i\n",
    "LEFT JOIN current_products c ON i.product_id = c.product_id\n",
    "WHERE c.product_id IS NULL\n",
    "   OR (c.product_name <> i.product_name OR c.category <> i.category OR c.list_price <> i.list_price)\n",
    "\"\"\")\n",
    "\n",
    "#check products SCD2\n",
    "display(\n",
    "  spark.table(\"silver_products_scd\")\n",
    "       .orderBy(\"product_id\",\"valid_from\")\n",
    ")\n",
    "# Tip: also filter current only:\n",
    "# display(spark.table(\"silver_products_scd\").where(\"is_current = true\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0112883c-ee31-45b8-b7a2-8040f05d4513",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 5 – Silver Orders & Order Items (Append + Clean)\n",
    "\n",
    "**What this does:**  \n",
    "We lightly **clean** today’s orders and line items, then **append** them into Silver tables. This keeps facts simple and fast: new transactions are treated as **new rows**.\n",
    "\n",
    "**How it works:**  \n",
    "- Cast IDs and numerics to the right types.  \n",
    "- Trim and normalize strings (upper-case `status`).  \n",
    "- Filter out bad rows (e.g., `quantity > 0`).  \n",
    "- **Append** to `silver_orders` and `silver_order_items` (no MERGE, no dedupe).\n",
    "\n",
    "**Why append-only here:**  \n",
    "Orders/items are modeled as **events**. For this project, we’re not tracking order status history or overwriting prior rows. \n",
    "\n",
    "**Expected outcome:**  \n",
    "New rows are added to `silver_orders` and `silver_order_items`, ready to be joined in Gold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "85bdd2dc-76f7-4e75-bcdd-f93bb540e356",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 5: Silver Orders & Items (simple append-clean)\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Create target if not exists\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS silver_orders (\n",
    "  order_id INT, customer_id INT, order_date STRING, status STRING\n",
    ") USING delta\n",
    "\"\"\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS silver_order_items (\n",
    "  order_item_id INT, order_id INT, product_id INT, quantity INT, unit_price DOUBLE\n",
    ") USING delta\n",
    "\"\"\")\n",
    "\n",
    "# Clean incoming and append to targets\n",
    "o_inc = (spark.table(\"bronze_orders_inc\")\n",
    "          .withColumn(\"order_id\", F.col(\"order_id\").cast(\"int\"))\n",
    "          .withColumn(\"customer_id\", F.col(\"customer_id\").cast(\"int\"))\n",
    "          .withColumn(\"order_date\", F.trim(F.col(\"order_date\").cast(\"string\")))\n",
    "          .withColumn(\"status\", F.upper(F.trim(F.col(\"status\"))))\n",
    "        )\n",
    "\n",
    "i_inc = (spark.table(\"bronze_order_items_inc\")\n",
    "          .withColumn(\"order_item_id\", F.col(\"order_item_id\").cast(\"int\"))\n",
    "          .withColumn(\"order_id\",      F.col(\"order_id\").cast(\"int\"))\n",
    "          .withColumn(\"product_id\",    F.col(\"product_id\").cast(\"int\"))\n",
    "          .withColumn(\"quantity\",      F.col(\"quantity\").cast(\"int\"))\n",
    "          .withColumn(\"unit_price\",    F.col(\"unit_price\").cast(\"double\"))\n",
    "          .filter(F.col(\"quantity\") > 0)\n",
    "        )\n",
    "\n",
    "# Append (no dedupe here)\n",
    "(o_inc.write.mode(\"append\").format(\"delta\").saveAsTable(\"silver_orders\"))\n",
    "(i_inc.write.mode(\"append\").format(\"delta\").saveAsTable(\"silver_order_items\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9a56ed36-1c4b-4768-af80-1f75623a61c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 6 – Gold Fact (Point-in-Time Join to Product SCD)\n",
    "\n",
    "**What this does:**  \n",
    "Builds the **line-item fact** table and attaches the **correct product version as of the order date** (PIT = point-in-time).\n",
    "\n",
    "**How it works:**  \n",
    "- Load Silver tables: items (`i`), orders (`o`), customers (`c`), and product SCD2 (`p_scd`).  \n",
    "- **Parse `order_date` safely** with regex-guarded formats → `order_date_parsed` (a true `DATE`).  \n",
    "- Join `i`→`o`→`c` normally, then **PIT-join to products** using:\n",
    "  - `i.product_id = p_scd.product_id` **and**\n",
    "  - `order_date_parsed BETWEEN valid_from AND valid_to` (open-ended on `valid_to`).  \n",
    "- Select metrics and attributes (e.g., `line_amount = quantity * unit_price`) and **write** to `gold.fact_order_item_pti`.\n",
    "\n",
    "**Why this matters:**  \n",
    "Because products can change (price/category), a simple join could attach the wrong attributes. The **PIT window** guarantees we use the **version that was active on the order date**.\n",
    "\n",
    "**Expected outcome:**  \n",
    "`gold.fact_order_item_pti` with one row per order item, enriched with **time-correct product attributes**, ready for daily KPI aggregation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af806970-d3a8-4ff2-be92-aa0de13a3f54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_item_id</th><th>order_id</th><th>customer_id</th><th>product_id</th><th>order_date</th><th>quantity</th><th>unit_price</th><th>line_amount</th><th>product_name</th><th>category</th><th>list_price</th></tr></thead><tbody><tr><td>117</td><td>46</td><td>1</td><td>3</td><td>2024-12-07</td><td>2</td><td>74.07</td><td>148.14</td><td>null</td><td>null</td><td>null</td></tr><tr><td>118</td><td>46</td><td>1</td><td>9</td><td>2024-12-07</td><td>5</td><td>103.42</td><td>517.1</td><td>null</td><td>null</td><td>null</td></tr><tr><td>119</td><td>46</td><td>1</td><td>22</td><td>2024-12-07</td><td>2</td><td>66.79</td><td>133.58</td><td>null</td><td>null</td><td>null</td></tr><tr><td>120</td><td>46</td><td>1</td><td>14</td><td>2024-12-07</td><td>1</td><td>84.03</td><td>84.03</td><td>null</td><td>null</td><td>null</td></tr><tr><td>124</td><td>49</td><td>1</td><td>20</td><td>2024-04-24</td><td>5</td><td>135.94</td><td>679.7</td><td>null</td><td>null</td><td>null</td></tr><tr><td>125</td><td>49</td><td>1</td><td>10</td><td>2024-04-24</td><td>4</td><td>352.16</td><td>1408.64</td><td>null</td><td>null</td><td>null</td></tr><tr><td>126</td><td>49</td><td>1</td><td>4</td><td>2024-04-24</td><td>4</td><td>339.97</td><td>1359.88</td><td>null</td><td>null</td><td>null</td></tr><tr><td>153</td><td>61</td><td>3</td><td>14</td><td>2025-04-26</td><td>1</td><td>84.03</td><td>84.03</td><td>null</td><td>null</td><td>null</td></tr><tr><td>154</td><td>61</td><td>3</td><td>7</td><td>2025-04-26</td><td>5</td><td>113.23</td><td>566.15</td><td>null</td><td>null</td><td>null</td></tr><tr><td>155</td><td>61</td><td>3</td><td>12</td><td>2025-04-26</td><td>5</td><td>142.7</td><td>713.5</td><td>null</td><td>null</td><td>null</td></tr><tr><td>156</td><td>61</td><td>3</td><td>25</td><td>2025-04-26</td><td>4</td><td>443.3</td><td>1773.2</td><td>null</td><td>null</td><td>null</td></tr><tr><td>165</td><td>64</td><td>2</td><td>18</td><td>2024-07-16</td><td>4</td><td>193.06</td><td>772.24</td><td>null</td><td>null</td><td>null</td></tr><tr><td>175</td><td>68</td><td>2</td><td>3</td><td>2024-04-28</td><td>3</td><td>74.07</td><td>222.20999999999998</td><td>null</td><td>null</td><td>null</td></tr><tr><td>176</td><td>68</td><td>2</td><td>25</td><td>2024-04-28</td><td>4</td><td>443.3</td><td>1773.2</td><td>null</td><td>null</td><td>null</td></tr><tr><td>193</td><td>74</td><td>3</td><td>15</td><td>2025-03-22</td><td>1</td><td>173.42</td><td>173.42</td><td>null</td><td>null</td><td>null</td></tr><tr><td>242</td><td>92</td><td>3</td><td>15</td><td>2024-09-07</td><td>1</td><td>173.42</td><td>173.42</td><td>null</td><td>null</td><td>null</td></tr><tr><td>244</td><td>92</td><td>3</td><td>24</td><td>2024-09-07</td><td>2</td><td>415.56</td><td>831.12</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1001</td><td>1</td><td>1</td><td>2025-03-29</td><td>2</td><td>60.11</td><td>120.22</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1001</td><td>1</td><td>1</td><td>2025-03-29</td><td>2</td><td>60.11</td><td>120.22</td><td>null</td><td>null</td><td>null</td></tr><tr><td>1</td><td>1001</td><td>1</td><td>1</td><td>2025-03-29</td><td>2</td><td>60.11</td><td>120.22</td><td>null</td><td>null</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         117,
         46,
         1,
         3,
         "2024-12-07",
         2,
         74.07,
         148.14,
         null,
         null,
         null
        ],
        [
         118,
         46,
         1,
         9,
         "2024-12-07",
         5,
         103.42,
         517.1,
         null,
         null,
         null
        ],
        [
         119,
         46,
         1,
         22,
         "2024-12-07",
         2,
         66.79,
         133.58,
         null,
         null,
         null
        ],
        [
         120,
         46,
         1,
         14,
         "2024-12-07",
         1,
         84.03,
         84.03,
         null,
         null,
         null
        ],
        [
         124,
         49,
         1,
         20,
         "2024-04-24",
         5,
         135.94,
         679.7,
         null,
         null,
         null
        ],
        [
         125,
         49,
         1,
         10,
         "2024-04-24",
         4,
         352.16,
         1408.64,
         null,
         null,
         null
        ],
        [
         126,
         49,
         1,
         4,
         "2024-04-24",
         4,
         339.97,
         1359.88,
         null,
         null,
         null
        ],
        [
         153,
         61,
         3,
         14,
         "2025-04-26",
         1,
         84.03,
         84.03,
         null,
         null,
         null
        ],
        [
         154,
         61,
         3,
         7,
         "2025-04-26",
         5,
         113.23,
         566.15,
         null,
         null,
         null
        ],
        [
         155,
         61,
         3,
         12,
         "2025-04-26",
         5,
         142.7,
         713.5,
         null,
         null,
         null
        ],
        [
         156,
         61,
         3,
         25,
         "2025-04-26",
         4,
         443.3,
         1773.2,
         null,
         null,
         null
        ],
        [
         165,
         64,
         2,
         18,
         "2024-07-16",
         4,
         193.06,
         772.24,
         null,
         null,
         null
        ],
        [
         175,
         68,
         2,
         3,
         "2024-04-28",
         3,
         74.07,
         222.20999999999998,
         null,
         null,
         null
        ],
        [
         176,
         68,
         2,
         25,
         "2024-04-28",
         4,
         443.3,
         1773.2,
         null,
         null,
         null
        ],
        [
         193,
         74,
         3,
         15,
         "2025-03-22",
         1,
         173.42,
         173.42,
         null,
         null,
         null
        ],
        [
         242,
         92,
         3,
         15,
         "2024-09-07",
         1,
         173.42,
         173.42,
         null,
         null,
         null
        ],
        [
         244,
         92,
         3,
         24,
         "2024-09-07",
         2,
         415.56,
         831.12,
         null,
         null,
         null
        ],
        [
         1,
         1001,
         1,
         1,
         "2025-03-29",
         2,
         60.11,
         120.22,
         null,
         null,
         null
        ],
        [
         1,
         1001,
         1,
         1,
         "2025-03-29",
         2,
         60.11,
         120.22,
         null,
         null,
         null
        ],
        [
         1,
         1001,
         1,
         1,
         "2025-03-29",
         2,
         60.11,
         120.22,
         null,
         null,
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_item_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "order_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "customer_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "unit_price",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "line_amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "product_name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "list_price",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 6: Gold fact_order_item_pti (point-in-time product attributes)\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "i = spark.table(\"silver_order_items\")\n",
    "o = spark.table(\"silver_orders\") \\\n",
    "    .withColumn(\"order_date\", F.trim(F.col(\"order_date\").cast(\"string\"))) \\\n",
    "    .withColumn(\n",
    "        \"order_date_parsed\",\n",
    "        F.when(F.col(\"order_date\").rlike(r'^\\d{4}-\\d{2}-\\d{2}$'),\n",
    "               F.to_date(\"order_date\",\"yyyy-MM-dd\"))\n",
    "         .when(F.col(\"order_date\").rlike(r'^\\d{4}/\\d{2}/\\d{2}$'),\n",
    "               F.to_date(\"order_date\",\"yyyy/MM/dd\"))\n",
    "         .when(F.col(\"order_date\").rlike(r'^\\d{2}/\\d{2}/\\d{4}$'),\n",
    "               F.to_date(\"order_date\",\"MM/dd/yyyy\"))\n",
    "         .when(F.col(\"order_date\").rlike(r'^\\d{2}-\\d{2}-\\d{4}$'),\n",
    "               F.to_date(\"order_date\",\"dd-MM-yyyy\"))\n",
    "         .otherwise(F.lit(None).cast(\"date\"))\n",
    "    )\n",
    "c = spark.table(\"silver_customers_cdc\")\n",
    "p_scd = spark.table(\"silver_products_scd\")\n",
    "\n",
    "fact_pti = (\n",
    "    i.join(o, \"order_id\")\n",
    "     .join(c, \"customer_id\")\n",
    "     .join(\n",
    "        p_scd,\n",
    "        (i.product_id == p_scd.product_id) &\n",
    "        (o.order_date_parsed >= p_scd.valid_from) &\n",
    "        (o.order_date_parsed <  p_scd.valid_to),\n",
    "        \"left\"\n",
    "     )\n",
    "     .select(\n",
    "        \"order_item_id\",\"order_id\",\"customer_id\", i.product_id.alias(\"product_id\"),\n",
    "        o.order_date_parsed.alias(\"order_date\"),\n",
    "        \"quantity\",\"unit_price\",\n",
    "        (F.col(\"quantity\")*F.col(\"unit_price\")).alias(\"line_amount\"),\n",
    "        \"product_name\",\"category\",\"list_price\"\n",
    "     )\n",
    ")\n",
    "\n",
    "(fact_pti.write.mode(\"overwrite\").option(\"overwriteSchema\",\"true\")\n",
    " .format(\"delta\").saveAsTable(\"gold.fact_order_item_pti\"))\n",
    "\n",
    " #check gold fact table with point-in-time product attributes\n",
    "display(\n",
    "  spark.table(\"gold.fact_order_item_pti\")\n",
    "       .select(\"order_item_id\",\"order_id\",\"customer_id\",\"product_id\",\n",
    "               \"order_date\",\"quantity\",\"unit_price\",\"line_amount\",\n",
    "               \"product_name\",\"category\",\"list_price\")\n",
    "       .orderBy(\"order_id\",\"order_item_id\")\n",
    "       .limit(20)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f42685b6-bd8a-4cc4-8af7-3342c5b1cd00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 7 – Gold Daily Aggregate (Incremental Upsert)\n",
    "\n",
    "**What this does:**  \n",
    "Aggregates the line-item fact into **daily KPIs** (orders, items, revenue, distinct customers, AOV) and **upserts** results by `order_date` so we can re-run for new days without rebuilding everything.\n",
    "\n",
    "**How it works:**  \n",
    "- Group `gold.fact_order_item_pti` by `order_date` to compute metrics and `aov = revenue / orders_cnt`.  \n",
    "- Create the target table `gold.fact_daily_sales_inc` if it doesn’t exist.  \n",
    "- Use `MERGE` on `order_date`:\n",
    "  - **MATCHED** → `UPDATE` that day’s row (idempotent re-runs).  \n",
    "  - **NOT MATCHED** → `INSERT` a new day.\n",
    "\n",
    "**Why this matters:**  \n",
    "This demonstrates an **incremental pattern**: add or refresh only the affected days, which is how real pipelines avoid full recomputes.\n",
    "\n",
    "**Expected outcome:**  \n",
    "`gold.fact_daily_sales_inc` contains one row per day with stable KPIs. The display shows a clean time series we can chart (e.g., revenue by date)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be3b8d36-c6de-40c7-b31f-27701162a91e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>order_date</th><th>orders_cnt</th><th>items_cnt</th><th>revenue</th><th>distinct_customers</th><th>aov</th></tr></thead><tbody><tr><td>2024-04-24</td><td>1</td><td>3</td><td>3448.2200000000003</td><td>1</td><td>3448.22</td></tr><tr><td>2024-04-28</td><td>1</td><td>2</td><td>1995.41</td><td>1</td><td>1995.41</td></tr><tr><td>2024-07-16</td><td>1</td><td>1</td><td>772.24</td><td>1</td><td>772.24</td></tr><tr><td>2024-09-07</td><td>1</td><td>2</td><td>1004.54</td><td>1</td><td>1004.54</td></tr><tr><td>2024-12-07</td><td>1</td><td>4</td><td>882.85</td><td>1</td><td>882.85</td></tr><tr><td>2025-03-22</td><td>1</td><td>1</td><td>173.42</td><td>1</td><td>173.42</td></tr><tr><td>2025-03-29</td><td>2</td><td>8</td><td>1045.44</td><td>2</td><td>522.72</td></tr><tr><td>2025-04-26</td><td>1</td><td>4</td><td>3136.88</td><td>1</td><td>3136.88</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2024-04-24",
         1,
         3,
         3448.2200000000003,
         1,
         3448.22
        ],
        [
         "2024-04-28",
         1,
         2,
         1995.41,
         1,
         1995.41
        ],
        [
         "2024-07-16",
         1,
         1,
         772.24,
         1,
         772.24
        ],
        [
         "2024-09-07",
         1,
         2,
         1004.54,
         1,
         1004.54
        ],
        [
         "2024-12-07",
         1,
         4,
         882.85,
         1,
         882.85
        ],
        [
         "2025-03-22",
         1,
         1,
         173.42,
         1,
         173.42
        ],
        [
         "2025-03-29",
         2,
         8,
         1045.44,
         2,
         522.72
        ],
        [
         "2025-04-26",
         1,
         4,
         3136.88,
         1,
         3136.88
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "order_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "orders_cnt",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "items_cnt",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "revenue",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "distinct_customers",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "aov",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# STEP 7: Incremental daily aggregate with MERGE\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "f = spark.table(\"gold.fact_order_item_pti\")\n",
    "\n",
    "daily_new = (\n",
    "  f.where(F.col(\"order_date\").isNotNull())\n",
    "   .groupBy(\"order_date\")\n",
    "   .agg(\n",
    "     F.countDistinct(\"order_id\").alias(\"orders_cnt\"),\n",
    "     F.count(\"*\").alias(\"items_cnt\"),\n",
    "     F.sum(\"line_amount\").alias(\"revenue\"),\n",
    "     F.countDistinct(\"customer_id\").alias(\"distinct_customers\"),\n",
    "   )\n",
    "   .withColumn(\"aov\", F.round(F.col(\"revenue\")/F.when(F.col(\"orders_cnt\")!=0, F.col(\"orders_cnt\")), 2))\n",
    ")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS gold.fact_daily_sales_inc (\n",
    "  order_date DATE,\n",
    "  orders_cnt BIGINT,\n",
    "  items_cnt BIGINT,\n",
    "  revenue DOUBLE,\n",
    "  distinct_customers BIGINT,\n",
    "  aov DOUBLE\n",
    ") USING delta\n",
    "\"\"\")\n",
    "\n",
    "daily_new.createOrReplaceTempView(\"daily_new\")\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO gold.fact_daily_sales_inc AS tgt\n",
    "USING daily_new AS src\n",
    "ON tgt.order_date = src.order_date\n",
    "WHEN MATCHED THEN UPDATE SET *\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\")\n",
    "\n",
    "# check gold daily aggregate table\n",
    "display(\n",
    "  spark.table(\"gold.fact_daily_sales_inc\")\n",
    "       .orderBy(\"order_date\")\n",
    ")\n",
    "# Optional: in the output, click Chart → Line (X=order_date, Y=revenue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94811093-d671-4486-a297-2c80e7e0a2b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 8 – Data Quality (DQ) Checks  \n",
    "\n",
    "**What this does:**  \n",
    "We run simple **data quality checks** on the pipeline outputs to confirm that Silver and Gold tables are consistent and reliable.  \n",
    "\n",
    "**How it works:**  \n",
    "- **Duplicate customers** → check if any `customer_id` appears more than once in `silver_customers_cdc`.  \n",
    "- **Missing products** → check if any fact rows do not match a product in `silver_products_scd`.  \n",
    "- **Bad quantities** → check if any fact rows have `quantity <= 0`.  \n",
    "\n",
    "**Expected results:**  \n",
    "- Duplicate customers should be `0` (CDC logic keeps only one row per customer).  \n",
    "- Missing products should normally be `0` (every fact row should match to a valid product).  \n",
    "- Bad quantities should be `0` (we filtered invalid rows in Silver).\n",
    "\n",
    "**Why this matters:**  \n",
    "By comparing expected vs observed results, we can quickly spot mismatches and highlight potential data alignment or quality issues that need attention in real-world pipelines. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d5cc9fb-baa3-4d80-9062-a1f2fd771e4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|dup_customers|\n+-------------+\n|            0|\n+-------------+\n\n+----------------+\n|missing_products|\n+----------------+\n|              15|\n+----------------+\n\n+-------+\n|bad_qty|\n+-------+\n|      0|\n+-------+\n\n"
     ]
    }
   ],
   "source": [
    "# STEP 8: DQ checks (quick)\n",
    "spark.sql(\"SELECT COUNT(*) AS dup_customers FROM (SELECT customer_id FROM silver_customers_cdc GROUP BY customer_id HAVING COUNT(*)>1)\").show()\n",
    "spark.sql(\"SELECT COUNT(*) AS missing_products FROM gold.fact_order_item_pti f LEFT JOIN silver_products_scd p ON f.product_id=p.product_id WHERE p.product_id IS NULL\").show()\n",
    "spark.sql(\"SELECT COUNT(*) AS bad_qty FROM gold.fact_order_item_pti WHERE quantity <= 0\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5660961b-1ad4-402c-b8f3-8cbd923d139c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Observed results:**  \n",
    "- `dup_customers = 0` ✅  \n",
    "- `missing_products = 15` ⚠️ → 15 fact rows did not join to a product in `silver_products_scd`. This indicates orders referencing product IDs that do not have a valid version at the order date. In practice, such rows would be flagged for review or handled with a default product.  \n",
    "- `bad_qty = 0` ✅  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a306b4a7-f396-41b2-b8d4-7c6967527c4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 9 – Delta Lake History  \n",
    "\n",
    "**What this does:**  \n",
    "We use Delta Lake’s built-in **time travel and audit history** feature to review all operations on a table.  \n",
    "\n",
    "**How it works:**  \n",
    "`DESCRIBE HISTORY silver_products_scd` shows a log of changes including:  \n",
    "- Operation type (e.g., `MERGE`, `INSERT`, `UPDATE`).  \n",
    "- User, timestamp, and environment details.  \n",
    "- Version numbers, which can be queried directly with `VERSION AS OF` or `TIMESTAMP AS OF`.  \n",
    "\n",
    "**Expected results:**  \n",
    "A table of history entries for `silver_products_scd`. At minimum, we see the initial inserts (Day 1 load). If additional runs are executed with updates, `MERGE` operations appear in the log.  \n",
    "\n",
    "**Why this matters:**  \n",
    "Delta’s history provides an **audit trail** and supports **time travel queries**, making debugging and reproducibility easier in production pipelines.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2bb8731-433d-4d6a-a8d3-9c7d1c9c6696",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>version</th><th>timestamp</th><th>userId</th><th>userName</th><th>operation</th><th>operationParameters</th><th>job</th><th>notebook</th><th>clusterId</th><th>readVersion</th><th>isolationLevel</th><th>isBlindAppend</th><th>operationMetrics</th><th>userMetadata</th><th>engineInfo</th></tr></thead><tbody><tr><td>4</td><td>2025-09-29T07:13:50.000Z</td><td>71920865700348</td><td>nagpalsahil0205@gmail.com</td><td>WRITE</td><td>Map(mode -> Append, statsOnLoad -> false, partitionBy -> [])</td><td>null</td><td>List(2782975292225066)</td><td>0929-063326-qb7q466z-v2n</td><td>3</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 0, numOutputRows -> 0, numOutputBytes -> 0)</td><td>null</td><td>Databricks-Runtime/17.1.x-aarch64-photon-scala2.13</td></tr><tr><td>3</td><td>2025-09-29T07:13:48.000Z</td><td>71920865700348</td><td>nagpalsahil0205@gmail.com</td><td>MERGE</td><td>Map(predicate -> [\"((product_id#13162 = product_id#13116) AND is_current#13168)\"], clusterBy -> [], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [])</td><td>null</td><td>List(2782975292225066)</td><td>0929-063326-qb7q466z-v2n</td><td>2</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 0, numTargetBytesAdded -> 0, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 1306, materializeSourceTimeMs -> 881, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 349, numTargetRowsUpdated -> 0, numOutputRows -> 0, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 0, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 0)</td><td>null</td><td>Databricks-Runtime/17.1.x-aarch64-photon-scala2.13</td></tr><tr><td>2</td><td>2025-09-29T06:59:08.000Z</td><td>71920865700348</td><td>nagpalsahil0205@gmail.com</td><td>WRITE</td><td>Map(mode -> Append, statsOnLoad -> true, partitionBy -> [])</td><td>null</td><td>List(2782975292225066)</td><td>0929-063326-qb7q466z-v2n</td><td>1</td><td>WriteSerializable</td><td>false</td><td>Map(numFiles -> 1, numOutputRows -> 3, numOutputBytes -> 2008)</td><td>null</td><td>Databricks-Runtime/17.1.x-aarch64-photon-scala2.13</td></tr><tr><td>1</td><td>2025-09-29T06:59:06.000Z</td><td>71920865700348</td><td>nagpalsahil0205@gmail.com</td><td>MERGE</td><td>Map(predicate -> [\"((product_id#12607 = product_id#12561) AND is_current#12613)\"], clusterBy -> [], matchedPredicates -> [{\"actionType\":\"update\"}], statsOnLoad -> false, notMatchedBySourcePredicates -> [], notMatchedPredicates -> [])</td><td>null</td><td>List(2782975292225066)</td><td>0929-063326-qb7q466z-v2n</td><td>0</td><td>WriteSerializable</td><td>false</td><td>Map(numTargetRowsCopied -> 0, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 0, numTargetBytesAdded -> 0, numTargetBytesRemoved -> 0, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 0, executionTimeMs -> 957, materializeSourceTimeMs -> 291, numTargetRowsInserted -> 0, numTargetRowsMatchedDeleted -> 0, numTargetDeletionVectorsUpdated -> 0, scanTimeMs -> 585, numTargetRowsUpdated -> 0, numOutputRows -> 0, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 0, numTargetFilesRemoved -> 0, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 0)</td><td>null</td><td>Databricks-Runtime/17.1.x-aarch64-photon-scala2.13</td></tr><tr><td>0</td><td>2025-09-29T06:59:03.000Z</td><td>71920865700348</td><td>nagpalsahil0205@gmail.com</td><td>CREATE TABLE</td><td>Map(partitionBy -> [], clusterBy -> [], description -> null, isManaged -> true, properties -> {\"delta.enableDeletionVectors\":\"true\"}, statsOnLoad -> false)</td><td>null</td><td>List(2782975292225066)</td><td>0929-063326-qb7q466z-v2n</td><td>null</td><td>WriteSerializable</td><td>true</td><td>Map()</td><td>null</td><td>Databricks-Runtime/17.1.x-aarch64-photon-scala2.13</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         4,
         "2025-09-29T07:13:50.000Z",
         "71920865700348",
         "nagpalsahil0205@gmail.com",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]",
          "statsOnLoad": "false"
         },
         null,
         [
          "2782975292225066"
         ],
         "0929-063326-qb7q466z-v2n",
         3,
         "WriteSerializable",
         false,
         {
          "numFiles": "0",
          "numOutputBytes": "0",
          "numOutputRows": "0"
         },
         null,
         "Databricks-Runtime/17.1.x-aarch64-photon-scala2.13"
        ],
        [
         3,
         "2025-09-29T07:13:48.000Z",
         "71920865700348",
         "nagpalsahil0205@gmail.com",
         "MERGE",
         {
          "clusterBy": "[]",
          "matchedPredicates": "[{\"actionType\":\"update\"}]",
          "notMatchedBySourcePredicates": "[]",
          "notMatchedPredicates": "[]",
          "predicate": "[\"((product_id#13162 = product_id#13116) AND is_current#13168)\"]",
          "statsOnLoad": "false"
         },
         null,
         [
          "2782975292225066"
         ],
         "0929-063326-qb7q466z-v2n",
         2,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "1306",
          "materializeSourceTimeMs": "881",
          "numOutputRows": "0",
          "numSourceRows": "0",
          "numTargetBytesAdded": "0",
          "numTargetBytesRemoved": "0",
          "numTargetChangeFilesAdded": "0",
          "numTargetDeletionVectorsAdded": "0",
          "numTargetDeletionVectorsRemoved": "0",
          "numTargetDeletionVectorsUpdated": "0",
          "numTargetFilesAdded": "0",
          "numTargetFilesRemoved": "0",
          "numTargetRowsCopied": "0",
          "numTargetRowsDeleted": "0",
          "numTargetRowsInserted": "0",
          "numTargetRowsMatchedDeleted": "0",
          "numTargetRowsMatchedUpdated": "0",
          "numTargetRowsNotMatchedBySourceDeleted": "0",
          "numTargetRowsNotMatchedBySourceUpdated": "0",
          "numTargetRowsUpdated": "0",
          "rewriteTimeMs": "0",
          "scanTimeMs": "349"
         },
         null,
         "Databricks-Runtime/17.1.x-aarch64-photon-scala2.13"
        ],
        [
         2,
         "2025-09-29T06:59:08.000Z",
         "71920865700348",
         "nagpalsahil0205@gmail.com",
         "WRITE",
         {
          "mode": "Append",
          "partitionBy": "[]",
          "statsOnLoad": "true"
         },
         null,
         [
          "2782975292225066"
         ],
         "0929-063326-qb7q466z-v2n",
         1,
         "WriteSerializable",
         false,
         {
          "numFiles": "1",
          "numOutputBytes": "2008",
          "numOutputRows": "3"
         },
         null,
         "Databricks-Runtime/17.1.x-aarch64-photon-scala2.13"
        ],
        [
         1,
         "2025-09-29T06:59:06.000Z",
         "71920865700348",
         "nagpalsahil0205@gmail.com",
         "MERGE",
         {
          "clusterBy": "[]",
          "matchedPredicates": "[{\"actionType\":\"update\"}]",
          "notMatchedBySourcePredicates": "[]",
          "notMatchedPredicates": "[]",
          "predicate": "[\"((product_id#12607 = product_id#12561) AND is_current#12613)\"]",
          "statsOnLoad": "false"
         },
         null,
         [
          "2782975292225066"
         ],
         "0929-063326-qb7q466z-v2n",
         0,
         "WriteSerializable",
         false,
         {
          "executionTimeMs": "957",
          "materializeSourceTimeMs": "291",
          "numOutputRows": "0",
          "numSourceRows": "0",
          "numTargetBytesAdded": "0",
          "numTargetBytesRemoved": "0",
          "numTargetChangeFilesAdded": "0",
          "numTargetDeletionVectorsAdded": "0",
          "numTargetDeletionVectorsRemoved": "0",
          "numTargetDeletionVectorsUpdated": "0",
          "numTargetFilesAdded": "0",
          "numTargetFilesRemoved": "0",
          "numTargetRowsCopied": "0",
          "numTargetRowsDeleted": "0",
          "numTargetRowsInserted": "0",
          "numTargetRowsMatchedDeleted": "0",
          "numTargetRowsMatchedUpdated": "0",
          "numTargetRowsNotMatchedBySourceDeleted": "0",
          "numTargetRowsNotMatchedBySourceUpdated": "0",
          "numTargetRowsUpdated": "0",
          "rewriteTimeMs": "0",
          "scanTimeMs": "585"
         },
         null,
         "Databricks-Runtime/17.1.x-aarch64-photon-scala2.13"
        ],
        [
         0,
         "2025-09-29T06:59:03.000Z",
         "71920865700348",
         "nagpalsahil0205@gmail.com",
         "CREATE TABLE",
         {
          "clusterBy": "[]",
          "description": null,
          "isManaged": "true",
          "partitionBy": "[]",
          "properties": "{\"delta.enableDeletionVectors\":\"true\"}",
          "statsOnLoad": "false"
         },
         null,
         [
          "2782975292225066"
         ],
         "0929-063326-qb7q466z-v2n",
         null,
         "WriteSerializable",
         true,
         {},
         null,
         "Databricks-Runtime/17.1.x-aarch64-photon-scala2.13"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "version",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "timestamp",
            "nullable": true,
            "type": "timestamp"
           },
           {
            "metadata": {},
            "name": "userId",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "userName",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "operation",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "operationParameters",
            "nullable": true,
            "type": {
             "keyType": "string",
             "type": "map",
             "valueContainsNull": true,
             "valueType": "string"
            }
           },
           {
            "metadata": {},
            "name": "job",
            "nullable": true,
            "type": {
             "fields": [
              {
               "metadata": {},
               "name": "jobId",
               "nullable": true,
               "type": "string"
              },
              {
               "metadata": {},
               "name": "jobName",
               "nullable": true,
               "type": "string"
              },
              {
               "metadata": {},
               "name": "jobRunId",
               "nullable": true,
               "type": "string"
              },
              {
               "metadata": {},
               "name": "runId",
               "nullable": true,
               "type": "string"
              },
              {
               "metadata": {},
               "name": "jobOwnerId",
               "nullable": true,
               "type": "string"
              },
              {
               "metadata": {},
               "name": "triggerType",
               "nullable": true,
               "type": "string"
              }
             ],
             "type": "struct"
            }
           },
           {
            "metadata": {},
            "name": "notebook",
            "nullable": true,
            "type": {
             "fields": [
              {
               "metadata": {},
               "name": "notebookId",
               "nullable": true,
               "type": "string"
              }
             ],
             "type": "struct"
            }
           },
           {
            "metadata": {},
            "name": "clusterId",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "readVersion",
            "nullable": true,
            "type": "long"
           },
           {
            "metadata": {},
            "name": "isolationLevel",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "isBlindAppend",
            "nullable": true,
            "type": "boolean"
           },
           {
            "metadata": {},
            "name": "operationMetrics",
            "nullable": true,
            "type": {
             "keyType": "string",
             "type": "map",
             "valueContainsNull": true,
             "valueType": "string"
            }
           },
           {
            "metadata": {},
            "name": "userMetadata",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "engineInfo",
            "nullable": true,
            "type": "string"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 15
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "version",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "timestamp",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "userId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "userName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operation",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "operationParameters",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "job",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"jobId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobName\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobRunId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"runId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"jobOwnerId\",\"nullable\":true,\"type\":\"string\"},{\"metadata\":{},\"name\":\"triggerType\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        },
        {
         "metadata": "{}",
         "name": "notebook",
         "type": "{\"fields\":[{\"metadata\":{},\"name\":\"notebookId\",\"nullable\":true,\"type\":\"string\"}],\"type\":\"struct\"}"
        },
        {
         "metadata": "{}",
         "name": "clusterId",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "readVersion",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "isolationLevel",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "isBlindAppend",
         "type": "\"boolean\""
        },
        {
         "metadata": "{}",
         "name": "operationMetrics",
         "type": "{\"keyType\":\"string\",\"type\":\"map\",\"valueContainsNull\":true,\"valueType\":\"string\"}"
        },
        {
         "metadata": "{}",
         "name": "userMetadata",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "engineInfo",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- STEP 9: Delta history (run as SQL cell or wrap with spark.sql)\n",
    "DESCRIBE HISTORY silver_products_scd;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb39ff16-d5f0-4998-8014-f3aefc0ae9c8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Step 10 (Optional) – Simulate Day 2 Incremental Data  \n",
    "\n",
    "**Objective:**  \n",
    "This step demonstrates how a **second daily feed** (Day 2) introduces both **changes** and **new records**, which will trigger CDC and SCD logic when we re-run Steps 3–7. It shows how the pipeline handles evolving data over time.  \n",
    "\n",
    "**How it works:**  \n",
    "- **Customers**:  \n",
    "  - `customer_id=2` → region changed from `SA` → `VIC` (CDC Type-1 update).  \n",
    "  - `customer_id=4` → brand new customer.  \n",
    "- **Products**:  \n",
    "  - `product_id=2` → price updated from `141.14` → `149.99` (SCD2 new version).  \n",
    "  - `product_id=4` → brand new product.  \n",
    "- **Orders and Order Items**:  \n",
    "  - Two new orders (`1003`, `1004`) and their items are added, referencing the changed/new customers and products.  \n",
    "- These are written to the Bronze `_inc` tables, overwriting the previous Day 1 increment.  \n",
    "\n",
    "**Next steps:**  \n",
    "To apply Day 2, we re-run **Steps 3–7**:  \n",
    "- Step 3 → merge customers (CDC).  \n",
    "- Step 4 → merge products (SCD2).  \n",
    "- Step 5 → append orders and items.  \n",
    "- Step 6 → rebuild fact with point-in-time product join.  \n",
    "- Step 7 → update daily aggregates.  \n",
    "\n",
    "**Expected outcome:**  \n",
    "After re-running, we should see:  \n",
    "- `silver_customers_cdc` updated so `customer_id=2` region = VIC, and `customer_id=4` inserted.  \n",
    "- `silver_products_scd` with a closed version of product 2 (old price) and a new row (new price), plus `product_id=4` inserted as current.  \n",
    "- `gold.fact_order_item_pti` and `gold.fact_daily_sales_inc` reflecting the new Day 2 orders.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93ef86db-f393-4ad5-a66a-23c05c0fe0b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# STEP 10: Simulate Day 2 (changes + new data)\n",
    "cust_day2 = spark.createDataFrame([\n",
    "    (2, \"Oliver Martin\", \"VIC\", \"2022-06-13\"),  # region changed SA -> VIC\n",
    "    (4, \"Zoe Kumar\",     \"QLD\", \"2024-01-29\"),  # new customer\n",
    "], [\"customer_id\",\"customer_name\",\"region\",\"signup_date\"])\n",
    "\n",
    "prod_day2 = spark.createDataFrame([\n",
    "    (2, \"Sports Item 2\", \"SPORTS\", 149.99),     # price changed 141.14 -> 149.99\n",
    "    (4, \"Electronics 4\", \"ELECTRONICS\", 339.97) # new product\n",
    "], [\"product_id\",\"product_name\",\"category\",\"list_price\"])\n",
    "\n",
    "orders_day2 = spark.createDataFrame([\n",
    "    (1003, 2, \"2025-03-30\", \"SHIPPED\"),\n",
    "    (1004, 4, \"2025-03-30\", \"SHIPPED\"),\n",
    "], [\"order_id\",\"customer_id\",\"order_date\",\"status\"])\n",
    "\n",
    "items_day2 = spark.createDataFrame([\n",
    "    (3,1003,2,1,149.99),   # uses new product price\n",
    "    (4,1004,4,1,339.97),   # brand new product\n",
    "], [\"order_item_id\",\"order_id\",\"product_id\",\"quantity\",\"unit_price\"])\n",
    "\n",
    "# Overwrite Bronze incrementals with Day 2\n",
    "cust_day2.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze_customers_inc\")\n",
    "prod_day2.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze_products_inc\")\n",
    "orders_day2.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze_orders_inc\")\n",
    "items_day2.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"bronze_order_items_inc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e2b32e2e-8a72-40d2-a5e8-44369a96906b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Conclusion & Learnings  \n",
    "\n",
    "In this project, we extended the basic medallion architecture from Project 1 into a more **realistic incremental pipeline** using Databricks, PySpark, and Delta Lake.  \n",
    "\n",
    "**Key learnings and skills demonstrated:**  \n",
    "- **Incremental ingestion**: simulated daily feeds with Bronze `_inc` tables.  \n",
    "- **CDC (Type-1)**: kept customers up-to-date by overwriting changed attributes.  \n",
    "- **SCD Type-2**: tracked product attribute changes over time with `valid_from`, `valid_to`, and `is_current`.  \n",
    "- **Point-in-time joins**: ensured orders were linked to the correct product version valid on the order date.  \n",
    "- **Incremental aggregation with MERGE**: refreshed daily KPIs without rebuilding the whole dataset.  \n",
    "- **Data quality checks**: verified duplicates, missing joins, and invalid values.  \n",
    "- **Delta Lake features**: used schema evolution and history for auditability and reproducibility.  \n",
    "\n",
    "This showcases the transition from **analyst-style batch cleaning** (Project 1) to **data engineering skills** such as incremental ETL, change data capture, and dimension versioning.  \n",
    "\n",
    "**Outcome:**  \n",
    "We now have a complete pipeline that ingests daily e-commerce data, applies CDC/SCD logic, and produces Gold-level fact and aggregate tables ready for analytics.  "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4510207648266851,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "DE_proj_2 2025-09-30",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}